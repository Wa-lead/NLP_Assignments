{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path, file, save=True):\n",
    "    \"\"\"\n",
    "    Tokenize a text file to characters and save it to a new file.\n",
    "    \"\"\"\n",
    "    data_set_type = path.split('/')[1]\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        # split the text into characters\n",
    "        text = list(text)\n",
    "        # reconstruct the text\n",
    "        text = ' '.join(text)\n",
    "        if save:\n",
    "            with open(f'{path}/{data_set_type}_tok/{file}.tok', 'w') as f:\n",
    "                f.write(text)\n",
    "        return f'{path}/{data_set_type}_tok/{file}.tok'\n",
    "\n",
    "# load all train corpora\n",
    "all_train_text = os.listdir('Europarl/train')\n",
    "# remove unwanted files\n",
    "all_train_text.remove('Icon_')\n",
    "all_train_text.remove('train_tok')\n",
    "\n",
    "# tokenize all train corpora\n",
    "for file in all_train_text:\n",
    "    tokenize(\"Europarl/train\",file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied heavy smoothing so it can be more robust to unseen grams of the the same language test. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower order is chosen given that the task is not to predict the next word, but to predict the language, which is not order and structure of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "def train(train_base):\n",
    "    corpus, lang = train_base.split(\".\")[:2]\n",
    "    command = f\"ngram-count -text {train_base} -order 2 -lm LMs/euro_LMs/{lang}.lm -addsmooth 10\"\n",
    "    os.system(command)\n",
    "    \n",
    "        \n",
    "train_base = os.listdir('Europarl/train/train_tok')\n",
    "for base in train_base:\n",
    "    train(f'Europarl/train/train_tok/{base}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions On Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# predict the language of a given text\n",
    "\n",
    "def predict(text):\n",
    "    # get the language models\n",
    "    lms = os.listdir('LMs/euro_LMs')\n",
    "    dir1, dir2 = text.split('/')[:2]\n",
    "    path = f'{dir1}/{dir2}'\n",
    "    file = text.split('/')[-1]\n",
    "    text = tokenize(path,file, save=True)\n",
    "    time.sleep(1)\n",
    "    ppls = []\n",
    "    for lm in lms:\n",
    "        # get the language\n",
    "        lang = lm.split('.')[0]\n",
    "        # get the ppl of the text given the language model\n",
    "        ppl = subprocess.check_output(f\"ngram -lm LMs/euro_LMs/{lm} -ppl {text}\", shell=True)\n",
    "        # get the ppl\n",
    "        ppl = ppl.decode('utf-8').split('ppl=')[1].split(' ')[1]\n",
    "    \n",
    "        # append the ppl and language to the list\n",
    "        ppls.append((ppl, lang))\n",
    "    # sort the list by the pplabilities\n",
    "    ppls.sort(key=lambda x: x[0])\n",
    "    # return the language with the highest ppl\n",
    "    return ppls[0][1]\n",
    "\n",
    "\n",
    "# load all dev corpora\n",
    "devs = os.listdir('Europarl/dev')\n",
    "\n",
    "# remove unwanted files\n",
    "devs.remove('dev.gold')\n",
    "devs.remove('dev_tok')\n",
    "\n",
    "# sort to maintain order\n",
    "devs = sorted(devs, key=lambda x: int(x.split('.')[1]))\n",
    "\n",
    "y_pred = []\n",
    "for dev in devs:\n",
    "    y_pred.append(predict(f'Europarl/dev/{dev}'))\n",
    "    \n",
    "# load labels\n",
    "y_true = open('Europarl/dev/dev.gold', 'r').read().split('\\n')\n",
    "y_true = [x.split('\\t')[1] for x in y_true]\n",
    "\n",
    "# calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: ')\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it bg\n",
      "lt lt\n",
      "et et\n",
      "fr fr\n",
      "hu hu\n",
      "lv lv\n",
      "cs cs\n",
      "en en\n",
      "da da\n",
      "de el\n",
      "mt mt\n",
      "nl nl\n",
      "pl pl\n",
      "fi fi\n",
      "pt pt\n",
      "ro ro\n",
      "sk sk\n",
      "sl sl\n",
      "sv sv\n",
      "bg el\n",
      "el sl\n",
      "es es\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(y_true, y_pred):\n",
    "    print(i,j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions On Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load all test corpora\n",
    "test_files = os.listdir('Europarl/test')\n",
    "test_files.remove('test_tok')\n",
    "test_files = sorted(test_files, key=lambda x: int(x.split('.')[1]))\n",
    "\n",
    "y_pred = []\n",
    "for test in test_files:\n",
    "    y_pred.append(predict(f'Europarl/test/{test}'))\n",
    "    \n",
    "with open('Europarl/test/test.pred', 'w') as f:\n",
    "    for label, test_file in zip(y_pred, test_files):\n",
    "        f.write(f'{test_file}\\t{label}\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
