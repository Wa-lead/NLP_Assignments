{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading fd 3\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 2933681 types 17863\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:214356\n",
      "Statistics:\n",
      "1 17863 D1=0.532532 D2=1.06821 D3+=1.37723\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 767 assuming -p 1.5\n",
      "probing 837 assuming -r models -p 1.5\n",
      "trie    519 without quantization\n",
      "trie    467 assuming -q 8 -b 8 quantization \n",
      "trie    519 assuming -a 22 array pointer compression\n",
      "trie    467 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:214356\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:214356\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "lmplz\t3091628032\t28770304\tRSSMax:3091628032 kB\tuser:0.225326\tsys:0.412387\tCPU:0.637721\treal:0.774059\n",
      "Reading LMs/1/lm.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "/Users/waleedalasad/Documents/GitHub/NLP_Assignments/assignment-2/kenlm/lm/model.cc:100 in void lm::ngram::detail::GenericModel<lm::ngram::detail::HashedSearch<lm::ngram::BackoffValue>, lm::ngram::ProbingVocabulary>::InitializeFromARPA(int, const char *, const lm::ngram::Config &) [Search = lm::ngram::detail::HashedSearch<lm::ngram::BackoffValue>, VocabularyT = lm::ngram::ProbingVocabulary] threw FormatLoadException.\n",
      "This ngram implementation assumes at least a bigram model. Byte: 97\n",
      "ERROR\n"
     ]
    }
   ],
   "source": [
    "def build_model(n_gram, train_data_path):\n",
    "    !mkdir -p \"LMs\"/\"{n_gram}\"\n",
    "    !kenlm/build/bin/lmplz --text \"{train_data_path}\" --arpa \"LMs\"/\"{n_gram}\"/lm.arpa --order {n_gram} --discount_fallback  --verbose_header \n",
    "    !kenlm/build/bin/build_binary \"LMs\"/\"{n_gram}\"/lm.arpa \"LMs\"/\"{n_gram}\"/lm.binary\n",
    "    \n",
    "build_model(1, 'English-Mix/UNCorpus.train.tok')\n",
    "# build_model(2, 'English-Mix/UNCorpus.train.tok')\n",
    "# build_model(3, 'English-Mix/UNCorpus.train.tok')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram\n",
      "UNCorpus.test.tok\n",
      "/bin/bash: LMs/UNCorpus.test.tok/test_dataset.txt: No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LMs/UNCorpus.test.tok/results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m1-gram\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUNCorpus.test.tok\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m get_perplexity_and_OOVs(\u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mUNCorpus.test.tok\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUNCorpus.wiki.test.tok.tok\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m get_perplexity_and_OOVs(\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUNCorpus.wiki.test.tok.tok\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mget_perplexity_and_OOVs\u001b[0;34m(n_gram, test_data_path, models_path, overwrite_files, print_to_console)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# calculate and dump to a file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mkenlm/build/bin/query \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLMs\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{test_data_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{n_gram}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/lm.binary < \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{models_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{test_data_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/test_dataset.txt > \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{models_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{test_data_path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m/results.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodels_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mtest_data_path\u001b[39m}\u001b[39;49;00m\u001b[39m/results.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m     16\u001b[0m \u001b[39m# collect\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LMs/UNCorpus.test.tok/results.txt'"
     ]
    }
   ],
   "source": [
    "def get_perplexity_and_OOVs(\n",
    "    n_gram:int,\n",
    "    test_data_path:str,\n",
    "    models_path='LMs',\n",
    "    overwrite_files=False,\n",
    "    print_to_console=True,\n",
    "):\n",
    "    !mkdir -p \"{models_path}\"/\"{n_gram}\"/\"{test_data_path}\"\n",
    "\n",
    "    # calculate and dump to a file\n",
    "    !kenlm/build/bin/query \"LMs\"/\"{test_data_path}\"/\"{n_gram}\"/lm.binary < \"{models_path}\"/\"{test_data_path}\"/test_dataset.txt > \"{models_path}\"/\"{test_data_path}\"/results.txt\n",
    "\n",
    "    with open(f\"{models_path}/{test_data_path}/results.txt\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # collect\n",
    "    perplexity_with_OOVs_line = lines[-4]\n",
    "    perplexity_without_OOVs_line = lines[-3]\n",
    "    counts_of_OOVs_line = lines[-2]\n",
    "\n",
    "    perplexity_with_OOVs = float(perplexity_with_OOVs_line.split(\"Perplexity including OOVs:\")[-1].strip())\n",
    "    perplexity_without_OOVs = float(perplexity_without_OOVs_line.split(\"Perplexity excluding OOVs:\")[-1].strip())\n",
    "    counts_of_OOVs = int(counts_of_OOVs_line.split(\"OOVs:\")[-1].strip())\n",
    "\n",
    "    return perplexity_with_OOVs, perplexity_without_OOVs, counts_of_OOVs\n",
    "\n",
    "print(\"1-gram\")\n",
    "print(\"UNCorpus.test.tok\")\n",
    "get_perplexity_and_OOVs(1, 'UNCorpus.test.tok')\n",
    "print(\"UNCorpus.wiki.test.tok.tok\")\n",
    "get_perplexity_and_OOVs(1, 'UNCorpus.wiki.test.tok.tok')\n",
    "\n",
    "print(\"2-gram\")\n",
    "print(\"UNCorpus.test.tok\")\n",
    "get_perplexity_and_OOVs(2, 'UNCorpus.test.tok')\n",
    "print(\"UNCorpus.wiki.test.tok.tok\")\n",
    "get_perplexity_and_OOVs(2, 'UNCorpus.wiki.test.tok.tok')\n",
    "\n",
    "print(\"3-gram\")\n",
    "print(\"UNCorpus.test.tok\")\n",
    "get_perplexity_and_OOVs(3, 'UNCorpus.test.tok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
